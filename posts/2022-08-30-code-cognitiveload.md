---
title:  Cognitive Loads in Programming 
lastmodified: Aug 30, 2022
featured: true
summary:  Easy to implement is not the same as simple to understand. Programming wants to be empirical it needs to be more deductive. 
toc: true
tags: patterns-of-erroneous-code
---


This post presents programming in a different light than what is commonly considered.  We will look at cognitive aspects of interacting with code.  

We will examine cognitive effort that goes into the implementation and cognitive loads on these poor souls who need to work on that code later.  Cognitive load of code consumption is much more interesting and less intuitive than the effort of its creation.   
We will think about bugs as cognitive overload.  We will discuss cognitive impact of abstractions, types, and programming principles.  
  
Cognitive load of working with code is rarely considered, how often do we hear a statement like
this? 

> &emsp; _"Yeah, I can do it but it will be complicated so I think we should not go there."_

I had quite a few eye opening moments when thinking about these topics. 
This is the main reason I decided to write and share my thoughts.
This post will be a high level rant discussing programming across the industry spectrum from JavaScript to Haskell. 

My pet peeve is identifying specific [patterns of erroneous code](/tags/patterns-of-erroneous-code.html) and what could be causing them, there is obviously a human factor underlying these patterns.   
Mental processes involved in writing code are such a fascinating and broad subject. 
I am taking only a very narrow path trough it.   
I am planning another high level post to discuss programming from a different but relevant angle: 
It will be about empirical and deductive approach to coding.   
I believe these 2 approaches come with different mindsets and impact our cognitive loads in interesting ways. 

This post reflects on my personal observations accumulated over 27 years of professional programming work. 
I am not a psychologist, these are observations of a coder.


## Cognitive psychology

Cognitive load theory defines cognitive load as the amount of information that working memory holds at one time. 
The idea is that the human brain is limited in that capacity.  Psychologist have identified the load to be about 7 "units of information". 
If you are into certain technical sports like skiing, swimming, golf...,
you may know first hand how hard it is to control just 2 aspects of your body movement at the same time. This space appears to be quite limited. I imagine the magic number is << 7 in programming. 

Cognitive load theory is concerned with _instructional design_ and improving how the _information is presented_ to a _learner_. 
Controlling learner's cognitive loads is an essential part of this work.   

Continuous learning is a part of being working programmer, but it is not the biggest mental effort. 
Implementing and modifying project code is by far the biggest cognitive effort that programmers face.   
I look at this as: the code itself is a very important instructional material, programmers are _learners_ and _instructional designers_ at the same time.    
Programs are where the _presentation of information_ happens.  
The concepts of cognitive load theory seem still relevant after this adjustment.  
  
Cognitive psychology considers 3 types of cognitive load: _Intrinsic, Extraneous, Germane_. 
All are related to information presentation and we will think about them in the context of code.  

* _Intrinsic cognitive load_ is the inherent level of difficulty associated with a specific (instructional) topic. 
Thinking about code, requirements are a good example of a topic. 
A rough moral equivalent known to software developers is _essential complexity_ (things are complex and there is nothing we can do about it).  

* _Extraneous cognitive load_ is generated by the manner in which information is presented to learners and is under the control of instructional designers.  This term is often used to describe unnecessary (artificially induced) cognitive load.  Thinking about code, a rough moral equivalent of high extraneous load is 
_accidental complexity_[^accidental] (things are complex because we made them so). 

* Germane cognitive load refers to the work that is put into constructing a long-lasting store of knowledge or schema.  Schema is a pattern of thought or behavior that organizes categories of information and the relationships among them.  Psychologist also use the term 
_"chunk"_ (pattern or concept with coarser granularity) and schema construction is the process of creating these chunks in memory.  
Thinking about code, this roughly translates to using _abstractions_, higher level concepts, types, programming principles.  An _OO_ programmer may try to establish intuitive object hierarchies to model the business domain, 
an FP-ier will may use denotational[^denotational] approach, look how things compose (category theory), design DSLs, blue-print the design using types. 

> &emsp; _Cognitive load theory thesis is about reducing extraneous cognitive load redirecting it towards germane load._

I need to emphasize that the _information presentation_ under consideration facilitates understanding of the code itself and not so much the concepts (e.g. abstractions) used to create it. 
Knowledge of these concepts is a prerequisite.  Prerequisites are an important caveat and one that ends but being contentious.  

_side_note_start
**Time for first reality check.** Working on a project code will reinforce knowledge of the abstractions (psychologist call something similar a [worked-example effect](https://en.wikipedia.org/wiki/Worked-example_effect)) but, for a working programmer, learning programming concepts ideally needs to happen outside of project work. In reality, there is no time for this. 
Also, abstractions that can be used in the program are limited not only by what the developer and the team know, but also by what is supported by the PL (programming language).  Developer backgrounds and 
what is supported in the PL vary a great deal. 
_side_note_end

[^denotational]: Denotational approach maps requirements to mathematical concepts such as Monoids, identifies categorical structures (things that compose), etc. [Conal Elliott's talk](https://www.youtube.com/watch?v=bmKYiUOEo2A) about it.  

[^accidental]:  Accidental and essential complexity have been introduced in [No Silver Bullet](https://en.wikipedia.org/wiki/No_Silver_Bullet)


This sets the stage for what I want do discuss, but before continuing *let me briefly review a few more relevant concepts.*  

_Cognitive overload_ happens when working memory is overwhelmed by the 3 cognitive loads we have described, IMO, bugs are evidence of cognitive overload in programming.  


I wanted to use _cognitive debt_ in the title, intending it as a pun on "technical debt", because I am interested in discussing
negative impacts on the team's ability to understand and reason about the code. 
However, this term turns out to have a clinical meaning and I decided against using it.   
_Cognitive debt_ is a psychological term associated with _repetitive negative thinking (RNT)_.
_Cognitive debt_ and RNT are hypothesized to have some very negative health consequences that can lead to depression or even dementia. 
RNT is described as 

> &emsp;  _“excessive and repetitive thinking about current concerns, problems, past experiences or worries about the future”_

I do not claim to know a lot about clinical psychology but the definition clearly is very relevant to programmers and could partially explain why programmers are often unhappy[^unhappy], why programming discussion groups are often very negative.   
Sadly, RNT seems to be a condition that really good programmers are likely to experience.  Good programmers think about rainy day scenarios, notice design flaws, can anticipate program issues.  My pet peeve, [patterns of erroneous code](/tags/patterns-of-erroneous-code.html), is an RNT.
It seems important that we talk about RNT.   


[^unhappy]: Interesting youtube: [Why Do So Many Programmers Lose Hope?](youtube.com/watch?v=NdA6aQR-s4U)

You may want to think about _working memory_ and _cognitive loads_ as something akin to _RAM_ in a computer.  
What is an equivalent of the CPU cost?   In this post I use _cognitive effort_, the usage of this term is not very consistent[^effort] in the literature.  

[^effort]: See the discussion in [A Computational Analysis of Cognitive Eﬀort](https://www.researchgate.net/publication/220963754_A_Computational_Analysis_of_Cognitive_Effort). The term _cognitive cost_ is typically used to mean a negative impact on a cognitive function induced by stress and the usage is also not very consistent, I am avoiding its use.

This were cliff notes written by a non expert.  There are many tricky and relevant bits like _information retrieval from long term memory_.  Things I am progressively less and less component to describe in psychological context.  


## That dreaded YAML

I am perusing thousands of lines in Infrastructure as Code (IAC) yaml files. I am looking at already refactored and improved version. It is a lot of templated _YAML_ of k8s configuration at my work.  The underlying reason for the complexity is the PL itself.   
Would something like _Dhall_[^dhall] be a better choice than _YAML_?
_Dhall_ allows to use ADTs, has strong type safety features, has lambda expressions. 
Programmer has access to these powerful, standard abstractions. I heard it being referred to as a "Non-Repetitive Alternative to YAML". I have used _Dhall_ enough to know that it is a solid product.

Are thousands of lines of templated yaml with lot of code duplication and no type safety simple to grasp?  Clearly not, YAML based IAC has a high extraneous load.
I could argue that the overall complexity of many _yaml_ files outweighs the cost of learning _Dhall_ with prerequisites.   

_side_note_start
**In real life,** _Dhall_ remains a niche approach, k8s configuration predominantly uses _YAML_. 
There could be many considerations at play here. E.g. IAC configuration needs to be accessible to all developers in the team _today_.  People have preferences on want they want to learn.  Some developer prefer to avoid a layer of indirection, if _YAML_ or _JS_ is used this is what they want to write. 
There are many angles on this and I am not going to even try to explore them.  
However, looking at just cognitive loads suggests giving _Dhall_ a consideration.   
This post main focus is the cognitive load and this example is here to demonstrate that there is more at play. 
_side_note_end

_YAML_ is what I would call easy, _Dhall_ is what I call simple.  

[^dhall]:  I you are not familiar with what _Dhall_ is, assume, for the sake of the argument, an existence of a configuration language 
that comes with a higher learning curve but offers much safer and reusable IAC, but it requires more effort to learn. 

## Simple vs Easy 

If you have looked at my [TypeScript types](/tags/TypeScript-Notes.html) series, you have seen me write about it [before](2022-03-13-ts-types-part6.html#about-simplicity). 
I do not claim that my definitions are the only correct way that these terms should be interpreted.  However, I have seen other programmers use a similar disambiguation so I am including it here.  

I consider the terms simple and easy to have a different meaning.   
Easy: The low barrier to entry (e.g. easy programming concepts). Hard is the opposite of easy and implies a high learning curve.   
Simple: Low effort to *correctly* reason about (e.g. code written using learned concepts). Complex is the opposite of simple (e.g. code that is hard to understand).  The term "arbitrary complexity" fits this definition very well. 

Easy means fewer prerequisites and implies low germane load, hard means more prerequisites.   
Simple means low extraneous load, complex means high extraneous load. 

This differentiation could also be expressed as: 

> &emsp;  _Easy means low cost of creation, simple means low cost of consumption_

except, in this post my interest is the cognitive effort only not the total cost.

Achieving _simplicity_ on a larger scale project is not _easy_. Easy does not scale well. 
There appears to be no free lunch, cognitive load needs to be somewhere.
My big preference is trying to achieve _hard and simple_ rather than _easy and complex_. 
Another words, I prefer to spend my cognitive bandwidth on germane load over extraneous load. 
This, I am pleased to note, is inline with cognitive psychology.   

Recall the advice from cognitive psychologists is to reduce extraneous load redirecting it towards germane load.
This translates to:

> &emsp; _Move from complex to hard_

An interesting way to look at easy vs simple is to think about a creative process like writing a book. 
Easy to write is clearly very different from simple to read. 
In programming these two are bundled together, a program created by one developer needs to be consumed by another dev
who needs to modify it or interact with it. 
The term "readable code" comes again to mind. I consider it different form simple.  E.g. readable code does not mean no subtle bugs. Message is readable if you know what it conveys but what it conveys could be complex or even misleading. 

IMO, the popularity of easy and the unpopularity of simple are a systemic problem in today’s programming and elsewhere.

Next section discusses examples of code which was kept easy and ended up complex.


## Extraneous loads that grow 

I was recently involved in a big rewrite of a JS application. 
It is one of these apps that can be described as: _was easy to write, is hard to maintain or understand_. 
I am sure very few readers will be surprised by existence of a hard to maintain JS application, but let's put taking about this aspect aside. 
Is writing "easy" code the same as generating excessive cognitive load for the maintainers?
I think it often can be, it is not that hard to incrementally develop a non penetrable maze. 
IMO, this is what will happen by default. 
Maintaining some code structure to manage the cognitive load is not "easy".    

Software is made out of many interacting pieces (granularity of statements, atomic computations, even lines of code).
This collection will not be easy to comprehend just because the pieces are easy to comprehend. 
The biggest contributors to the overall complexity are the interaction between the pieces, organization of the pieces, 
and not the pieces themselves.   

Mutating state is well known to be a terrible way to accomplish communication between parts of the code. 
My career worst in the mutation department was a Java Struts 1 code where a class had over 200 of mutating instance variables[^inheritance].  Changing order of 2 lines in this code was almost guaranteed to create a (typically intermittent) bug, it was hard to even make sure that variables were set before they were read.     
This code used no advanced concepts, all ingredients were easy: control statements, instance variables, lots of protected methods with typically no arguments and void returns that read and set the instance variables.  I consider it one of the most complex programs I worked with in my 27 years of professional programming.  
This code become infamous for its complexity very fast.  Interestingly, Struts were blamed, not the needless overuse of mutable state.    
Ability to program using clear inputs and outputs and immutable data requires a learning effort, I submit to you that this effort is lower than the cognitive effort of maintaining such code. Probably the _hardest_ bit is knowing which programming concepts to avoid. 

[^msgs]:  I believe this was the original idea behind OOP

[^inheritance]: actually, there were 2 classes with about 100 vars each, you got to use inheritance!

Just a moment ago, I wanted you to think about your program as a collection of many small pieces like statements or maybe lines of code.  This is a terrible way to comprehend a program!
There is just to much extraneous load at this level (I will discuss this more in [Bugs](#bugs) section).  Abstractions are needed!     
I call code without adequate abstraction a _brute force_. 
_Brute force code_ could be a large set of incrementally developed, well written, heterogeneous pieces which interact well but there are now just too many of them. Brute force code is something that can benefit from a more cohesive design.  
The idea here is that a more cohesive design reduces cognitive load.  Human cognitive load is limited but we can do abstract reasoning!  It is simpler for us to deal with a few generalized abstractions than with multiplicity of concretes.  
As a side note, concrete thinking is not always bad.  An interesting article on this in a broader context [Concrete Thinking: Building Block, Stumbling Block, or Both?](https://www.healthline.com/health/concrete-thinking). 
Benefits of abstraction are a recurring pattern in this post.  We are talking about progressively more prerequisites.  


Types can be very effective in reducing extraneous complexity.  It is much easier to comprehend types that it is to comprehend the whole program.  Types also aid defining and expressing abstractions. 
Many PLs provide enough support to benefit from explicit well defined types.
In my code, type declarations often take more space than the actual programs. We are talking about progressively more prerequisites.

Returning to the JS application I worked on. The final product is still close to JS (it uses TypeScript) but...
the new code has 4 simple ingredients: immutability, [referential transparency](2022-03-13-ts-types-part6.html#referential-transparency-purity-and-explicit-types), [clear types](2022-03-13-ts-types-part6.html#about-clarity), and _async/await_ abstraction to avoid callback hell. 
Not that many prerequisites to consume the new code! Referential transparency is an interesting dichotomy.  Experiencing different results every time the code is executed it typically a surprise, however developers typically do not think about this during implementation. 
Thus, the code may feel weird and opinionated (e.g. React components avoid using hooks) but a little weirdness is a small price to pay if anyone compares it to its predecessor. 

> &emsp; *IMO, high quality code shifts cognitive load from maintainer to implementer*   

This works great even if both are the same person.  My story about JS application rewrite shows that this is possible.  
In this case, the biggest prerequisite for the implementer was knowledge about what to avoid.


_side_note_start
**In a unicorn universe,** projects are not allowed to exceed certain thresholds of cognitive load.   
When the threshold is reached abstractions that lower the load are identified, learned, and applied. 
&#129412; 
_side_note_end


## Bugs 

Let's define a bug as an unintended program defect, that removes all the temporary hacks and "bugs that are features" from consideration.  But it is the programmer's job to figure these things out. A bug implies some issue in the mental process. 

Types can be very helpful in bug prevention.  Programmers who start using a PL with powerful types (e.g. Idris, Haskell) experience this first hand: lots of compilation errors, many uncovering an issue in the program.   
I started analyzing and recording all defects I encounter at work, even if it is an issue I introduced and fixed within the same commit.  My goal is to understand better what have caused and could have prevented each issue.  Almost all issue I encounter could have been prevented with better types.  However this judgment has some _hind-sight cognitive bias_ and likely is specific to my projects and PL tool capabilities. 

I consider cognitive overload to be the main cause of bugs. 
Thus, and if you agree, we should look for ways to reduce that load.   
That load could be intrinsic (complex requirements) or extraneous (a complexity under programmer's control).
My records suggest that more bugs fall into the second category. 
Cognitive psychology advice is to reduce cognitive load by redirecting extraneous load towards germane load.  That would suggest using more types and abstractions, moving towards higher level concepts.  This, obviously, assumes that these high level concepts themselves are not erroneous (to be discussed in next section).  

How about the typos, trivial overlooks that are sometimes so hard to spot?  That mysterious brain of ours is good at creating these. 
A great reading on this, in the context of (non-programming) typos, is 
[WUWT, Why It's So Hard to Catch Your Own Typos](https://www.wired.com/2014/08/wuwt-typos/).  
Human brain has ability to fill in gaps, self-correct things.  Human brain is better at focusing on 
high level ideas and is perfectly happy skipping over minute details.  This characteristics seems even stronger if we are on board with the big idea, and it seems fair to assume that programmers are on board with the features they are implementing. 
The main point is that our brain is not well designed to work at the level of statements and lexical tokens. 

_side_note_start
Side note:  This line of thought could also partially explain why programmers seem to be at home in the code they wrote even if other programmers consider it a complete mess. Sometimes just changing font or background color allows us to spot issues we have overlooked before. Our perception changes if what we interact with feels foreign (interestingly this should increase the cognitive load). 
It appears that some mental reset is sometimes needed. 
_side_note_end

Error proneness of programming at the level of PL statements is also consistent with the cognitive load theory.  At this level a programmer needs to consider a multitude of details, most likely overwhelming the working memory limits.   
An interesting piece of trivia is that Turing's original paper (the one about universal machines and halting problem) had several bugs in it.
_If Turing could not get it right, what chance do we have?_[^scott]  

[^scott]:  “If Turing could not get it right, what chance do we have?”- is something I heard in a lecture by Dana Scott.  
The bugs were only discovered during the actual implementation work ([Alan Turing](https://blog.wolframalpha.com/2010/06/23/happy-birthday-alan-turing/)).

The level of details I am talking about here (PL statements, lexical tokens) is often associated with 
dynamically typed PLs and imperative or even procedural programming. This is only partially true. 
Static compilation can prevent a lot of trivial errors and hopefully the prevented list will grow, but it is far from being exhaustive.  

**What are my points?**  

My first point is that programmers should start considering cognitive aspects of programming more. 

What is that we do when we discover a bug?  We write a test, right?  Does that decrease the cognitive load?  Of course it does not. Tests play important role but are not a Pavlov's stick.  Instead of adding a test, I often try to rethink types to make the code simpler and safer. 

_Metacognition_ is an important concept in cognitive psychology. It is about knowing strengths and weaknesses in your cognitive process.  This suggests some form of bug post mortem retrospective where we analyze the cause and think how the bug could have been prevented.  My suggestion is asking these questions:
Is the cause intrinsic or extraneous complexity?  Could this bug be prevented with better type safety? 
Could this bug be prevented with better abstractions? 

Here is an example that keeps popping in my mind when thinking about trivial errors.  I have seen many stack overlflow errors in my life, I have seen only 2 or 3 since I moved to Haskell.  They all were caused by Haskell allowing this lambda expression:

```Haskell
let blah = blah 
in blah
```

This to me is a good example of extraneous complexity that could be prevented by the compiler.  Many PLs (e.g. anything in ML groups like OCaml, Reason will not allow such code). 
Here is a relevant discussion on reddit: [NoRecursiveLet](https://www.reddit.com/r/haskell/comments/lxnbrl/ghc_proposal_norecursivelet_prevent_accidental/).
Thinking about reducing extraneous load could impact such discussion (in this case, by supporting the proposal). 

My second point is the recurring one, types and abstractions can play a big role in reducing bugs.  
Hopefully types and abstractions themselves are bug free!  


## Extraneous load of abstraction

Summary of previous episodes:  Our cognitive load is limited but we are capable of abstract reasoning. Abstractions seem like our best hope in reducing the overall code complexity. But ...there are a few caveats.  

**Poorly implemented abstractions**

You spotted an intermittent malfunction in your code. Thank God, you see only one commit in recent history and you have a strong hunch something is wrong with that commit. Only some 50 code changes. 
The one that caused the issue is: `var1 == var2` changed to `var2 == var1`.  Would you be able to spot it? 
I call this type of issue a "gotcha".   
How about, your _finder_ function seems to be not finding stuff, only that sounds too far fetched, the function looks correct, so you just ignore this as a possible explanation.  The underlying issue is
that sometimes `x =! x` and you have used equality check to find things. 

I like to think about this paraphrasing Gimli: 

> &emsp;  _"Abstractions are upon you, whether you would risk them or not."_ 

Equality is an example of an abstraction developers implement and use, but not think much about.  
However, the list of surprising behaviors like these is quite long affecting all kinds of abstractions. 
Gochas create chaos in the cognitive process. 
For abstractions to work as a cognitive load reducer, they need to be treated seriously.   

Developers I talked to often responded to such examples by saying something like: "This is just bad code, whoever implemented it should have been more careful". 
Except, I can point to examples in standard libraries of popular mainstream PLs or popular frameworks[^gotchas1].
The issues come with no deprecation warning and, if documented, are considered a 'feature'.   
Are questions like "does a developer have a fighting chance of troubleshooting this feature?" even being asked? 
Gotchas often become mystery bugs and are resolved using workarounds. 

[^gotchas1]: Example of non-symmetric equals is `java.sql.Timestamp` used with `java.sql.Date` or `java.util.Date`, these remain used as standard JDBC mappings for DB columns, the usage will show no deprecation warning.  `[] !== []` and
`[] != []` in JS (incidentally `[] == ""`), working in JS often feels like explosives engineering. 
I wrote a blog series about [TypeScript Types](/tags/TypeScript-Notes.html) and ended up presenting a lot of complexities and gotchas that probably surprise any TS developer.  
How do Hibernate users prevent this [concurrency issue](http://rpeszek.blogspot.com/2014/08/i-dont-like-hibernategrails-part-2.html)?  Java Streams have a very interesting take on referential transparency. 
If you execute a closure twice the second call will fail. This is probably the first and only attempt at dynamically typed linear types.


**Abstractions themselves causing issues**
 
OOP creates a very high cognitive load, to a point that even compiler writers mess it up all the time[^rust]. 
I started my programming career as an OOP enthusiast and evangelist. OO programming has an appeal of simplicity and I was seduced by it for many years.  It took me a long time to realize that OOP is not simple at all. 
Let's talk OOP a little. Pick a random OOP training. You will probably learn that _Cat_ *is a*n _Animal_ and that everything is intuitive.   
You will not learn if any of these less obvious are (or should be) true:  
&emsp; function accepting a _Cat_ *is a* function accepting an _Animal_  
&emsp; array of _Cats_ *is a*n array of _Animals_[^array]    
&emsp; function with no parameters *is a* function with one parameter[^function].  
You will not learn about reduced type safety that comes with widening to a superclass[^widening]. 
I do not even want to start on subtyping gotchas of variant (union and sum) types. 
OOP is approachable only because we hide the complex bits from the learners[^ts-variance].  
Relevant psychological concept is a cognitive bias called _framing effect_. 

[^array]:  Keeping things easy, arrays are mutable. Sadly, you can explore the answer on your own by asking a mainstream compiler like Java or TS 
and the answer will, unfortunately be the incorrect _yes_.

[^function]:  In TS and JS the answer is yes. In TS this is a subtyping rule.

[^widening]:  This is a gotcha generator especially in OOP languages that have some level of type inference. 
E.g. here are some gotchas in [TS](2021-12-12-ts-types-part1.html#compilation-bloopers) that involve widening to  `unknown` which is top type in TS, here is a discussion about these in [Scala](http://rpeszek.blogspot.com/2017/07/scala-whats-wrong-with-you_29.html).

[^ts-variance]:  For example, it is hard to imagine than the unsound implementation of variance in a PL like TS was accidental.  It must have been a decision to keep things easy.

[^rust]: "even compiler writers mess it up all the time" is a quote from ([Rust Subtyping Documentation](https://doc.rust-lang.org/nomicon/subtyping.html)) 

The concept of exception (i.e. `throw` and `catch` game) is another example of a risky complexity that impacts even Haskell[^non-termination].    
Types can reduce cognitive load of understanding the code, except exceptions provide a very accessible and virally used way to bypass the types. 
Other "bottom" types like `null` are in the same boat.   
I really like what Rust has done in this regard, you can _panic_ but it is hard to recover if you do, otherwise errors are handled in an `Either`-like sum type called `Result`.  Hopefully we will see more of this pragmatic approach in future PLs.

You may notice that the examples of _gotchas_ I am coming up with have something in common. These issues can be classified under: _not trustworthy types_.  Misleading types will confuse any developer, that includes developers who work in dynamically typed languages and may not think about types explicitly.

Are there any "gotcha" free environments?  Haskell comes close but is not perfect[^haskell]. 
Proof assistants like Idris come to mind, these can even verify totality.  That is kinda interesting, let's pause for a bit here...  Consider the levels of abstraction used in proof assistants. It appears that our brain needs something at the level of a dependently typed lambda calculus to work correctly[^ml]. 
That could make sense, for things to be logical you need, well you need the logic itself.      

[^non-termination]: I sometimes see this argument "It is impossible to statically reason about termination in a Turing complete PL, thus, all hope is lost".  Firstly, this is inaccurate: it is possible to statically verify totality on a subset of programs. Secondly: if non-termination is like accidentally hurting your foot, then exception is like shooting yourself in the foot.  Missing file should, IMO, rarely be treated as non-termination. (I use the terms _total_, _terminating_ and _partial_, *non_terminating* interchangeably.)

[^haskell]: Haskell dedicates a significant effort to soundness. E.g. see [Type Classes vs. the World](https://www.youtube.com/watch?v=hIZxTQP1ifo). 
Not everything is perfect however. 
Haskell allows for easy to abuse error non-termination (e.g. `error`, `undefined` functions), however ability to `catch` is more limited than in most PLs. Non-termination in itself throws a wrench, one Haskell should not be blamed for, see [Hask is not a category](http://math.andrej.com/2016/08/06/hask-is-not-a-category/) and 
[What Category do Haskell Types and Functions Live In](http://blog.sigfpe.com/2009/10/what-category-do-haskell-types-and.html).
Overall Haskell language comes with much fewer surprises if compared to the mainstream.    
Haskell ecosystem (including its standard library) are more lax than the language itself.  Michael Snoyman's [Haskell Bad Parts](https://www.snoyman.com/blog/2020/10/haskell-bad-parts-1/) is a great series on this topic. 
The most recent surprise for me is how _Aeson_ (the most popular Haskell library for dealing with JSON)
[generic instances work](https://github.com/haskell/aeson/issues/961). 

[^ml]: _Standard ML_ is known for its soundness, I do not know _ML_ family that well, but I do know it has exceptions and `throw/catch` (in this case `raise/handle`) games. Possibly a more accurate point here is that we need strict formal semantics, it does not need to be dependently typed. 

Some developers react to gotchas with something akin to _omission neglect_ (psychological concept loosely described by this popular phrase: _out of sight out of mind_), while other developers maintain a mental 
knowledge base of gotchas and their potential impacts.  I am in the second group.
I will also note a possible relationship to _repetitive negative thinking_.



**High levels of abstraction**

I have seen very abstract code where the abstraction was like trees preventing developer from noticing a forest. 
One source of such examples is error handling. 
Mathematics rarely concerns itself with error messages, falsehood is falsehood.  I have blogged about it in my posts about [Maybe Overuse](2021-01-17-maybe-overuse.html) and [Alternative and errors](2021-02-13-alternative.html).    

_side_note_start
Side note:  Probably not surprisingly, these were rather negatively received, heavily down-voted posts. 
The topic itself is very much a _repetitive negative thinking_. 
Incidentally, the negative comments mostly belonged in the general “what you are describing is just bad code, whoever wrote it should have been more careful" category.  I want to understand how code abstractions could promote erroneous code, my interest is in what makes people not careful.  
_side_note_end

One simple to explain and not very abstract example that still fits into this section is the `guard`[^guard] combinator in Haskell. 
I see it used and I also scratch my head when, say, a JSON parser error says only `"mempty"`. 
Possibly, some programmers think about the abstraction called `Alternative` when they should be thinking
about something like `MonadFail`, an abstraction that allows to specify error messages.   
Some of us really dig abstractions and are arguably very good at them.  I consider myself in that group. 
But we are kidding ourselves if we do not acknowledge that abstractions can also blind us. 
IMO the one thing we can do about it is to be aware.  More diligence + awareness is typically all it takes.

[^guard]: For readers not familiar with Haskell, `guard` allows to reject a computation 
based on a boolean expression. It is defined using a very general concept of `Alternative` and at this level of generality specifying error message is not possible. In real life I see it used with parsers and other computations that could really use an error message.  

Gotchas may look like fun but in real life are not very pleasant to be around.  
I plan to return to gotchas in next post as IMO developers interact with gotchas very differently.

_side_note_start
**There is a planet** (not in our galaxy) where all programming abstractions and types are treated with respect. Unsound abstractions and incorrect implementation are removed and replaced.  The cognitive effort of programming on this planet is low.  &#127776;
_side_note_end

## Germane cost of FP
 
I was learning FP while working as a Java / Groovy developer. 
It took me 8 years, I estimated about 7000 hours.  This effort included Category Theory, Types (my main interest), PLT, programming in bunch of FP languages. 
Given typical consulting rates for a senior Java dev that is close to a million dollar personal investment. And, I still had to learn a lot when I started my actual Haskell job.   

I probably have convinced you that either I am slow on the uptake or FP is just too hard.  My point is not that FP cannot be learned and applied incrementally, rather that there is a lot to learn and doing so within project timelines is not going to work well. 
The other point is that even though my learning effort was high, it was still lower then concurrent cognitive loads I was facing at work. 

How many programmers or how many CS college graduates, do you think, will understand how the following (mid-school?) formulas apply to programming?:

> $a^{(b + c)} = a ^b * a ^c$    
> $a^{(b * c)} = (a ^ b) ^ c$

Is understanding of pattern match and currying formulas more or less important than knowing, say, the `kubectr` command?  The answer I recommend is: both are important. 
To finish your assignment you have to know `kubectr`, to finish it well you would benefit from understanding the principles.  
Given limited resources "have to" wins over "benefit from" every time. 
Learning, especially learning the principles has to happen outside of the project work. 

There are 2 reasons why FP is hard.  One: it is simply hard (has a decent surface area but is also deep), two: it is different.  
It requires a shift in how developer thinks.  This shift is especially hard if the developer can only practice imperative 
skills at work. The tools we use impact our cognitive function. 

> &emsp; "It is not only the violin that shapes the violinist, we are all shaped by the tools we train ourselves to use, and in this respect programming languages have a devious influence: they shape our thinking habits."

The quote is from [Dijkstra letter to The University of Texas](https://chrisdone.com/posts/dijkstra-haskell-java/) protesting their Haskell -> Java curriculum change.  If you are into technical sports, you may have heard the term "muscle memory".  It is often harder to unlearn or adjust a body movement then learn a new one from scratch.  It is even harder to "own" the old movement and the new movement at the same time.  Psychologists also believe that unlearning is hard[^unlearning].   
The required mental shift for FP is the source of all kinds of additional problems.  It can form a communication barrier, it can divide the community and teams.  

[^unlearning]: see 2.1 section in [Unlearning before creating new knowledge: A cognitive process.](https://core.ac.uk/download/pdf/77240027.pdf)

Let's come back to the topic of learning FP so I can dig my hole a little deeper, here is one example. 
There is one line of code that made a huge impact on me (it is called the _Free Monad_ and is in Haskell):

```Haskell
data Free f a = MkFree (f (Free f a)) | Pure a 
```
 
I decided to dedicate a full summer to learning this line and it ended up taking longer than that.  There is actually quite a bit to learn here!  
For example, how does it relate to this line (looks very similar, just replace `Free` with `Fix` and drop one constructor):

```Haskell
newtype Fix f a = MkFix (f (Fix f a))
```

Or, what does _free_ mean, and can other things than monads be _free_?  Can `Free`-s with different `f`-s be combined?  If so are there easier and harder ways of combining them. What is _freer_? 
Also, how do I use it?  What are the available libraries (there were not that many back then)?  How to I use it DIY style?  How does it (and should it, back then I did not understood errorproness of partiality) relate to `try-catch` games?   
Effect systems (the main application of `Free`) are a very powerful programming tool, they can add a lot of structure and cognitive simplicity[^effect] to the code.  I use 2 of them at work, one of them we maintain. 
Effect systems allow to organize code into DSLs and interpreters.  This approach creates very high level of code reuse, testability, defines very explicit, self-documenting types. 
But, is it realistic to learn the concepts in a day or a week when starting a new project?  Imagine a programmer who uses Java for work exploring this knowledge.   

[^effect]: Any extraneous cognitive loads associated with effects?  Yes there are a few especially on the implementation side. 
Also like most other tools effects can be abused. I sometimes see a single DSL instruction interpreted directly to IO (more Haskell terminology here, IO is what we call a sin-bin) and used in a brute-force IO code.  This just adds cognitive overhead of effects without taking advantage of what they have to offer. 

**What are my points?**  
The learning process needs to be gradual and independent of current project work, even though some ability to use FP early
is essential. 
It has to be incremental, understanding is not all or nothing game. 
Learning FP while programming in a mainstream language is super hard, however there are some steps one can take to move forward, e.g. introduce an FP-like library[^fplibrary] (e.g. Java's vavr).  With that said there is no substitute for the real thing.  
The biggest obstacle will be getting other team members interested in the journey. 

[^fplibrary]: Some care is needed in selecting the library, I have heard about people experiencing stack overflow issues, In my TS/JS project, I had to implement my own immutable map because one provided by _immutable.js_ caused performance issues. 

There has been some discussion about making Haskell itself more accessible (e.g. [_add_blank_target Elementary Programming](https://www.michaelpj.com/blog/2021/01/02/elementary-programming.html))
and some library effort in this direction as well (e.g. [IHP](https://github.com/digitallyinduced/ihp)).  
Some development teams are organized by separating micro services with high level of abstraction from the rest.  
Some places separate a possibly very advanced implementation from a simple to use API (I believe Facebook's Haxl does it). 
Creating a progression from easy to hard is less trivial.   

FP teaches respect for principles. IMO, large part of this is can be presented to learners easily.  E.g. importance of immutability, referential transparency, computations obeying laws, type clarity.  Thinking about computations as something that can be 
understood changed my life as a programmer.

_side_note_start
**In a parallel universe** Alonso Church did not take a temporary break from lambda calculus and showed it to his student, Alan Turning.  The first computer hardware was based on SKI calculus. In that universe kids learn mathematics with proofs, imperative programming is considered a great addition after programmers learn the principles.  In that parallel universe 
software has very few bugs, but there is much less of it.  &#127756;
_side_note_end

## Low Code

This post operated on the assumption that there is no free lunch, the cognitive load needs to be somewhere. 
Is this a valid assumption? 

What is _low code_?  At work I am working on a Haskell infrastructure that supports "low code" Python development. 
I have been around the block for a long time and it is hard not to see _low code_ as part of a reoccurring pattern (RAD e.g. Borland C++ or Powerbuilder, frameworks like Ruby on RAILS or Grails). The industry keeps trying to simplify software development but there are takeoffs.
_No code_ seems like the lowest possible cognitive load ever.  It might be!  Unless, of cause, it produces incorrect results or not the outcome you want.  

Low code typically implies a very simplified opinionated development that tries to remove coding out of ...well coding.  
Low-code is more "done for you" rather than "do it yourself". 
In the extreme _no code_ case, the target user could be someone who never wrote a program before. 
This can be great if you can live with the opinions.  But what if your needs are even slightly different that what the low-code designers have envisioned?

I look at low-code/no-code as evolution of frameworks like Ruby on Rails or Grails. 
With a framework like Grails you can interactively scaffold a simple CRUD app that is ready to use!  But what happens if your needs grow
and the app has to become more involved.  Hey, you still have access to the generated code and can do whatever you want, you do not even need to scaffold to start.  I worked on 3 Grails projects, 2 were not a cookie cutter. These 2 were very hard to maintain.    
How can one remove coding out of app development, yet provide ability to do arbitrary customizations?
Arbitrary customization benefits from access to the code and from no opinions. 

The other issue is that is is hard to automate a rainy day.  The designer of low code needs to be able to think about the rainy day to start with.
Returning to my Grails experience: using Grails, for means, for example, that you need to accept a serious [concurrency issue](http://rpeszek.blogspot.com/2014/08/i-dont-like-hibernategrails-part-2.html) I mentioned above.  

AI solutions look interesting in this space but have the same (and probably amplified) concerns. 

The idea of distributing cognitive effort across different components is not new.  The terms "decoupling" or
"isolation of concerns" are in this space.  Low code is an idea of a very lopsided distribution in which most of the complexity 
falls onto the infrastructure.  
The big question remains on what do you do if requirements change in a way not foreseen by the low-code design?  
IMO, low-code can be an interesting choice if you can work closely with the team of devs who implemented and maintain it.  


## About stress 

Maintaining messy code can be stressful. Fortunately, projects like these become "infamous" very fast, and you get moral support from other team members.  That really helps. 
My advice is: be a source of such support if your colleagues end up working in messy code.  Few words of encouragement and acknowledgment of that hardship can go a long way.  
Also the information will slowly percolate up and the management may become more receptive to accept the cost of a big refactor or even a complete re-write. 

This post has advocated for code simplicity over ease of development. Thinking that you know how to write simple code and not being allowed to do so can be very frustrating.  Sometimes there is a good reasons why the code is kept in a certain way. 
Understanding why things are the way they are is often good enough to create acceptance and alleviate frustration.  However, examples like [How to stop functional programming](https://brianmckenna.org/blog/howtostopfp) come to mind.  The industry should try to strive a balance between accessibility and simplicity better than this.  
With micro-services being so popular one would expect more opportunities for some divide and concur where at least some of the code strives to be hard and simple.  Whats really hard is finding a places like that.  The job market for functional programming jobs is, frankly, dismal.  At the same time, languages like Haskell and Rust top the weekend use stats based on stackoverflow surveys[^weekend].  There must be quite a few frustrated programmers out there.  I have been in that position and I know it is mentally hard. 

[^weekend]: Repeating some of what I wrote [here](2022-03-13-ts-types-part6.html#about-simplicity):  Haskell was firmly in the first position for the stackoverflow weekend use statistics for several years. Here is one link: [_add_blank_target 2017](https://stackoverflow.blog/2017/02/07/what-programming-languages-weekends/).  In [_add_blank_target 2019](https://stackoverflow.blog/2019/10/28/research-update-coding-on-the-weekends/) Rust moved ahead of Haskell. 
The job ranking (based on the UK's [_add_blank_target IT Jobs Watch](https://www.itjobswatch.co.uk/jobs/uk/haskell.do)) puts Haskell at 932 as of 2022/02/06.  Haskell moved ahead of COBOL in that ranking in 2017. 
This ranking is possibly exaggerated too, lots of jobs list Haskell as good to have but will have you code in PHP.  This bias exist
in any language but is stronger for something like Haskell than say COBOL. 

How do you cope with problems you cannot do anything about?  You have to find some way to stay positive. 
The big helpers are openness and empathy.   
Openness is the key, it is helpful to share our though process and to try to learn how out teammates think. 
We all are learning and there should be no shame in not knowing everything.  
It is OK to say "I do not understand this", even more than that, such statement should be encouraged. 
By saying it we do 3 things: accept our own limitations, say that we accept limitations of other team members, and move towards a more open team environment.  We also allow others to help,  helping others is a very positive and meaningful experience that, among other things, can reduce stress.

## There is much more to it

This post took a very narrow path through the very broad subject of cognitive aspects of programming.  

My focus was coding rather than process. I did not discuss things like keeping pool requests small, git hygiene, etc. 
 
Some PLs (Haskell is a good example of this) suffer from what some people call the _Lisp curse_.  Instead of using established libraries one-off tools are often created.  It is interesting why this happens and what to do about it.  Is this a case where programmers think about abstractions more than about libraries? 
From the cognitive load perspective, is writing it from scratch a lower effort than learning and applying existing solutions? 

Cognitive load should be viewed as a resource problem, one that does not scale very well, and one that is not well understood. 
Cognitive load is greatly impacted by turn over rates, switching of code ownership, and by installed processes. 
Context switching is very expensive, the programmer inability to find contiguous blocks of time to focus could be viewed as an indication of an under-resourced project.  

Linting, formatting, aesthetics are all very interesting topics as well.  Most programmers seem to be very sensitive to how the code is presented, (e.g. would you ever use a light background in your code editor?). 

Syntax vs semantics, it seems syntax has a huge cognitive role even it we may think of it as bikeshed. 

Habit formation and unlearning are a big topic. 

Cognitive biases in the context of coding seem like very interesting topic too. In particular _bandwagon effect_ (TypeScript is popular and hence must be very good), _framing effect_ (new cool technology), _commitment bias_ (we done it like this before), _functional fixedness_ (we do not need another PL?), _omission neglect_ (things we do not know are not important), _bikesheding_ (possibly most of this post &#128578;).

One topic I do plan to discuss (in future post) is a distinction between empirical and formal process in programming and how it impacts cognitive loads. 

This post did not run out of topics, rather I have run out of steam.  I hope, I gave you things to think about.  Thank you for reading! 

&#128521; 	 	&#128522;  	&#128578;
<font size="5rem"> &#129412; </font>	